---
title: "AI Provider Routing"
description: "AI provider routing strategies for performance, cost, and reliability"
---

## What is AI Provider Routing?

AI Provider Routing determines how Conductor connects to AI providers (OpenAI, Anthropic, Workers AI, Groq). The right routing mode can dramatically improve performance, reduce costs, and increase reliability.

<CardGroup cols={3}>
  <Card title="cloudflare" icon="bolt">
    Platform-native Workers AI with ultra-low latency
  </Card>

  <Card title="cloudflare-gateway" icon="network-wired">
    AI Gateway with caching, analytics, and cost controls
  </Card>

  <Card title="direct" icon="arrow-right">
    Direct API calls to OpenAI, Anthropic, Groq, etc.
  </Card>
</CardGroup>

## Three Routing Modes

### 1. Cloudflare (Platform-Native)

**For:** Workers AI models running on Cloudflare's network

```yaml
- member: generate-summary
  type: Think
  config:
    provider: workers-ai
    model: "@cf/meta/llama-3.1-8b-instruct"
    routing: cloudflare  # Platform-native
    temperature: 0.7
```

**Characteristics:**
- ‚ö° **Ultra-fast** - Sub-10ms latency to model
- üí∞ **Cost-effective** - Cloudflare's pricing (often free tier)
- üîê **No API keys** - Uses Workers AI binding
- üåç **Edge execution** - Runs closest to your users
- üì¶ **Smaller models** - 7B-70B parameter range

**When to use:**
- Latency is critical (< 50ms cold start)
- Cost optimization for high-volume workloads
- Simple tasks (summarization, classification, extraction)
- No external API key management desired

**Limitations:**
- Only Workers AI models available
- Smaller context windows than GPT-4/Claude
- Less sophisticated reasoning for complex tasks

### 2. Cloudflare Gateway (Recommended)

**For:** OpenAI, Anthropic, Groq through AI Gateway

```yaml
- member: generate-analysis
  type: Think
  config:
    provider: anthropic
    model: claude-3-5-sonnet-20241022
    routing: cloudflare-gateway  # Route through AI Gateway
    temperature: 0.7
```

**Characteristics:**
- üóÑÔ∏è **Persistent cache** - Cache spans deployments and users
- üìä **Real-time analytics** - Dashboard for costs, latency, errors
- üíµ **Cost controls** - Set spending limits and rate limits
- üîÑ **Retry logic** - Automatic retries with exponential backoff
- üåê **Multi-provider** - OpenAI, Anthropic, Groq, etc.

**When to use:**
- **Always** for production AI calls (unless using Workers AI)
- Need caching across requests and deployments
- Want visibility into AI spending and usage
- Multiple environments (dev, staging, prod)
- A/B testing different models or prompts

**Benefits:**

<AccordionGroup>
  <Accordion title="Persistent Caching" icon="database">
    Cache survives deployments. First user pays, everyone else benefits. Can reduce costs by 90%+ for repeated queries.
  </Accordion>

  <Accordion title="Analytics Dashboard" icon="chart-line">
    Real-time metrics on:
    - Cache hit rates
    - Request volume
    - Cost per model
    - Latency percentiles
    - Error rates by provider
  </Accordion>

  <Accordion title="Cost Controls" icon="dollar-sign">
    Set hard limits:
    - Max spend per day/month
    - Rate limits per user
    - Alert thresholds
    - Budget allocation by environment
  </Accordion>

  <Accordion title="A/B Testing" icon="flask">
    Split traffic between:
    - Different models (GPT-4 vs Claude)
    - Different prompts
    - Different temperatures
    - Track which performs better
  </Accordion>
</AccordionGroup>

### 3. Direct

**For:** Direct API calls bypassing AI Gateway

```yaml
- member: generate-code
  type: Think
  config:
    provider: openai
    model: o1-preview
    routing: direct  # Bypass gateway
    temperature: 1.0
```

**Characteristics:**
- üéØ **Direct connection** - No intermediary
- üÜï **Latest features** - Provider-specific capabilities
- üîß **Full control** - All provider parameters available
- ‚ùå **No gateway benefits** - No cache, analytics, or limits

**When to use:**
- Testing new provider features not yet in gateway
- Provider-specific parameters needed
- Debugging provider-specific issues
- Very low request volume (caching not beneficial)

**Tradeoffs:**
- Miss out on persistent caching
- No analytics or cost controls
- Manual retry logic needed
- Direct API keys required

## Configuration Examples

### Workers AI (Platform-Native)

```yaml
name: edge-classification

flow:
  - member: classify-intent
    type: Think
    config:
      provider: workers-ai
      model: "@cf/meta/llama-3.1-8b-instruct"
      routing: cloudflare
      systemPrompt: "Classify user intent into: question, request, or complaint."
```

**Setup required:**
```toml
# wrangler.toml
[ai]
binding = "AI"
```

### OpenAI via Gateway

```yaml
name: generate-article

flow:
  - member: write-draft
    type: Think
    config:
      provider: openai
      model: gpt-4o
      routing: cloudflare-gateway
      temperature: 0.7
      maxTokens: 2000
```

**Setup required:**
```toml
# wrangler.toml
[[ai.gateway]]
id = "my-gateway"
cache_ttl = 3600
```

```bash
# Environment variable
export OPENAI_API_KEY="sk-..."
```

### Anthropic via Gateway

```yaml
name: analyze-sentiment

flow:
  - member: detect-sentiment
    type: Think
    config:
      provider: anthropic
      model: claude-3-5-sonnet-20241022
      routing: cloudflare-gateway
      temperature: 0.3
```

**Setup required:**
```bash
# Environment variable
export ANTHROPIC_API_KEY="sk-ant-..."
```

### Groq via Gateway

```yaml
name: fast-extraction

flow:
  - member: extract-entities
    type: Think
    config:
      provider: groq
      model: llama-3.1-70b-versatile
      routing: cloudflare-gateway  # Ultra-fast with caching
      temperature: 0.2
```

**Setup required:**
```bash
# Environment variable
export GROQ_API_KEY="gsk_..."
```

### Direct OpenAI (No Gateway)

```yaml
name: test-new-model

flow:
  - member: try-o1
    type: Think
    config:
      provider: openai
      model: o1-preview
      routing: direct  # Bypass gateway for testing
      temperature: 1.0
```

## Multi-Model Strategies

### Cascade Pattern

Start with fast/cheap model, escalate to powerful model if needed:

```yaml
flow:
  # Try fast edge model first
  - member: quick-answer
    config:
      provider: workers-ai
      model: "@cf/meta/llama-3.1-8b-instruct"
      routing: cloudflare
    scoring:
      thresholds:
        minimum: 0.7
      onFailure: continue  # Don't abort if low quality

  # Escalate to powerful model if needed
  - member: detailed-answer
    condition: ${quick-answer.scoring.score < 0.7}
    config:
      provider: anthropic
      model: claude-3-5-sonnet-20241022
      routing: cloudflare-gateway
```

### Load Balancing

Distribute requests across providers:

```yaml
flow:
  - member: generate-summary
    config:
      # Alternate between providers
      provider: ${input.requestId % 2 === 0 ? 'openai' : 'anthropic'}
      model: ${input.requestId % 2 === 0 ? 'gpt-4o' : 'claude-3-5-sonnet-20241022'}
      routing: cloudflare-gateway
```

### Fallback Pattern

Try primary provider, fallback to secondary if unavailable:

```yaml
flow:
  - member: generate-content
    config:
      provider: openai
      model: gpt-4o
      routing: cloudflare-gateway
    retry:
      maxAttempts: 2
      onFailure: continue

  # Fallback if OpenAI fails
  - member: generate-content-fallback
    condition: ${!generate-content.success}
    config:
      provider: anthropic
      model: claude-3-5-sonnet-20241022
      routing: cloudflare-gateway
```

### Cost Optimization

Use cheaper models for simple tasks, expensive for complex:

```yaml
flow:
  # Classification with edge model (cheap)
  - member: classify-intent
    config:
      provider: workers-ai
      model: "@cf/meta/llama-3.1-8b-instruct"
      routing: cloudflare

  # Complex reasoning with flagship model (expensive)
  - member: generate-strategy
    condition: ${classify-intent.output.intent === 'complex'}
    config:
      provider: anthropic
      model: claude-3-5-sonnet-20241022
      routing: cloudflare-gateway
```

## Performance Comparison

### Latency

| Routing Mode | Cold Start | Warm | Cache Hit |
|--------------|-----------|------|-----------|
| cloudflare | < 50ms | < 10ms | < 5ms |
| cloudflare-gateway | < 100ms | 500-2000ms | < 5ms |
| direct | < 100ms | 500-2000ms | N/A |

### Cost (per 1K tokens)

| Provider | Model | Approx Cost |
|----------|-------|-------------|
| Workers AI | llama-3.1-8b | $0.001 |
| OpenAI | gpt-4o | $0.005 |
| OpenAI | gpt-4o-mini | $0.0002 |
| Anthropic | claude-3.5-sonnet | $0.003 |
| Groq | llama-3.1-70b | $0.0008 |

**With Gateway Cache (90% hit rate):**
- Effective cost: 10% of base cost
- Example: gpt-4o at $0.0005 per 1K tokens

### Reliability

| Routing Mode | Caching | Retry | Analytics | Rate Limiting |
|--------------|---------|-------|-----------|---------------|
| cloudflare | KV only | Manual | Basic | Platform |
| cloudflare-gateway | Persistent | Automatic | Full | Configurable |
| direct | None | Manual | None | Provider |

## Best Practices

### 1. Default to AI Gateway

```yaml
# ‚úÖ Use gateway for production
config:
  routing: cloudflare-gateway

# ‚ùå Don't use direct without reason
config:
  routing: direct
```

### 2. Use Workers AI for Simple Tasks

```yaml
# ‚úÖ Edge model for classification
- member: classify-email
  config:
    provider: workers-ai
    routing: cloudflare

# ‚ùå Don't waste GPT-4 on simple tasks
- member: classify-email
  config:
    provider: openai
    model: gpt-4o  # Overkill
```

### 3. Configure Appropriate Cache TTL

```yaml
# ‚úÖ Long cache for stable queries
- member: summarize-docs
  config:
    routing: cloudflare-gateway
  cache:
    ttl: 86400  # 24 hours

# ‚úÖ Short cache for dynamic data
- member: analyze-live-feed
  config:
    routing: cloudflare-gateway
  cache:
    ttl: 60  # 1 minute
```

### 4. Monitor Gateway Analytics

```typescript
// Check dashboard regularly
// Cloudflare Dashboard -> AI Gateway
// - Cache hit rates (target: > 80%)
// - Error rates (target: < 1%)
// - Latency p95 (target: < 3s)
// - Cost trends
```

### 5. Use Environment-Specific Routing

```yaml
flow:
  - member: generate-text
    config:
      # Development: use cheap edge models
      # Production: use powerful models via gateway
      provider: ${env.ENVIRONMENT === 'production' ? 'anthropic' : 'workers-ai'}
      model: ${env.ENVIRONMENT === 'production' ? 'claude-3-5-sonnet-20241022' : '@cf/meta/llama-3.1-8b-instruct'}
      routing: ${env.ENVIRONMENT === 'production' ? 'cloudflare-gateway' : 'cloudflare'}
```

## Troubleshooting

### Gateway Not Caching

**Symptom:** Every request hits provider API

**Causes:**
1. Gateway not configured in wrangler.toml
2. Cache TTL set to 0
3. Requests have unique parameters (temperature varies)
4. Using streaming (not cacheable)

**Fix:**
```toml
# wrangler.toml
[[ai.gateway]]
id = "my-gateway"
cache_ttl = 3600  # Enable caching
```

### High Latency

**Symptom:** Requests taking > 5s

**Causes:**
1. Using direct routing (no edge optimization)
2. Cold start + large model
3. No caching enabled
4. Provider issues

**Fix:**
```yaml
# Use gateway for caching
config:
  routing: cloudflare-gateway
cache:
  ttl: 3600  # Enable KV cache too
```

### Rate Limit Errors

**Symptom:** 429 errors from provider

**Causes:**
1. Exceeding provider limits
2. No rate limiting in gateway
3. Burst traffic

**Fix:**
```toml
# Configure rate limits in gateway
[[ai.gateway]]
id = "my-gateway"
rate_limiting_requests_per_minute = 60
```

### Cost Overruns

**Symptom:** Higher than expected AI costs

**Causes:**
1. Low cache hit rate
2. Using expensive models unnecessarily
3. No spending limits

**Fix:**
```yaml
# Use edge models for simple tasks
- member: simple-task
  config:
    provider: workers-ai
    routing: cloudflare

# Set gateway spending limits via dashboard
# Cloudflare Dashboard -> AI Gateway -> Settings
```

## Related Documentation

<CardGroup cols={2}>
  <Card
    title="AI Gateway Docs"
    icon="network-wired"
    href="https://developers.cloudflare.com/ai-gateway"
  >
    Official Cloudflare AI Gateway documentation
  </Card>

  <Card
    title="Workers AI Docs"
    icon="bolt"
    href="https://developers.cloudflare.com/workers-ai"
  >
    Workers AI models and usage
  </Card>

  <Card
    title="Caching Guide"
    icon="database"
    href="/conductor/concepts/caching"
  >
    Learn about caching strategies
  </Card>

  <Card
    title="AI Integration Guide"
    icon="brain"
    href="/conductor/guides/ai-integration"
  >
    Complete guide to AI provider setup
  </Card>
</CardGroup>
