---
title: "Think Members"
description: "AI reasoning with Large Language Models"
---

## Overview

Think members execute AI reasoning tasks using Large Language Models (LLMs) from providers like OpenAI, Anthropic, Workers AI, and Groq. They're perfect for natural language understanding, content generation, classification, and complex reasoning.

<Note>
Think members don't require implementation code - Conductor handles AI provider calls automatically based on configuration.
</Note>

## Basic Configuration

```yaml
name: analyze-sentiment
type: Think
description: Analyze text sentiment

config:
  provider: openai
  model: gpt-4o-mini
  routing: cloudflare-gateway
  temperature: 0.3
  maxTokens: 100
  systemPrompt: |
    Analyze the sentiment of the given text.
    Respond with: positive, negative, or neutral.
```

```typescript
// members/analyze-sentiment/index.ts
// No implementation needed for Think members!
export default null;
```

## Configuration Options

### Provider Selection

```yaml
config:
  # OpenAI (GPT models)
  provider: openai
  model: gpt-4o

  # Anthropic (Claude models)
  provider: anthropic
  model: claude-3-5-sonnet-20241022

  # Workers AI (Edge models)
  provider: workers-ai
  model: "@cf/meta/llama-3.1-8b-instruct"

  # Groq (Ultra-fast inference)
  provider: groq
  model: llama-3.1-70b-versatile
```

### Routing Modes

```yaml
config:
  # AI Gateway (recommended) - caching + analytics
  routing: cloudflare-gateway

  # Platform-native (Workers AI only) - ultra-fast
  routing: cloudflare

  # Direct API calls - bypass gateway
  routing: direct
```

See [Routing Guide](/conductor/concepts/routing) for details.

### Temperature Control

```yaml
config:
  # Creative (varied output)
  temperature: 0.9

  # Balanced
  temperature: 0.7

  # Deterministic (consistent output)
  temperature: 0.1
```

### Token Limits

```yaml
config:
  # Short responses
  maxTokens: 100

  # Medium responses
  maxTokens: 500

  # Long responses
  maxTokens: 2000

  # Maximum (model-dependent)
  maxTokens: 4000
```

## System Prompts

### Basic System Prompt

```yaml
config:
  systemPrompt: |
    You are a helpful assistant that analyzes text sentiment.
    Always respond in JSON format.
```

### Structured Output

```yaml
config:
  systemPrompt: |
    Analyze the company and respond with JSON:
    {
      "industry": "string",
      "size": "small" | "medium" | "large",
      "confidence": number (0-1),
      "summary": "string"
    }
```

### Role-Based Prompts

```yaml
config:
  systemPrompt: |
    You are an expert business analyst with 20 years of experience.
    Analyze companies objectively and provide actionable insights.
    Focus on:
    - Financial health
    - Market position
    - Growth potential
    - Competitive advantages
```

### Few-Shot Prompts

```yaml
config:
  systemPrompt: |
    Classify customer feedback sentiment.

    Examples:
    Input: "I love this product! Best purchase ever!"
    Output: {"sentiment": "positive", "confidence": 0.95}

    Input: "It's okay, nothing special."
    Output: {"sentiment": "neutral", "confidence": 0.7}

    Input: "Terrible quality, waste of money."
    Output: {"sentiment": "negative", "confidence": 0.9}

    Now classify the following:
```

## Input Handling

### Simple String Input

```yaml
schema:
  input:
    type: object
    properties:
      text:
        type: string
```

Think member receives input directly:

```typescript
// Ensemble passes:
input: {
  text: "I love this product!"
}

// Think member uses it in prompt automatically
```

### Multiple Fields

```yaml
schema:
  input:
    type: object
    properties:
      companyName:
        type: string
      website:
        type: string
      industry:
        type: string
```

Fields are interpolated in system prompt:

```yaml
config:
  systemPrompt: |
    Analyze ${input.companyName} in the ${input.industry} industry.
    Website: ${input.website}
```

### Messages Array

```yaml
schema:
  input:
    type: object
    properties:
      messages:
        type: array
```

For multi-turn conversations:

```typescript
// Pass conversation history
input: {
  messages: [
    { role: "user", content: "What's the capital of France?" },
    { role: "assistant", content: "Paris" },
    { role: "user", content: "What's its population?" }
  ]
}
```

## Structured Output

### JSON Schema (OpenAI)

```yaml
config:
  provider: openai
  model: gpt-4o
  responseFormat:
    type: json_schema
    json_schema:
      name: company_analysis
      strict: true
      schema:
        type: object
        properties:
          industry:
            type: string
          employees:
            type: number
          founded:
            type: number
          summary:
            type: string
        required: [industry, employees, founded, summary]
        additionalProperties: false
```

### JSON Mode (General)

```yaml
config:
  responseFormat:
    type: json_object
  systemPrompt: |
    Respond only with valid JSON.
```

## Provider-Specific Features

### OpenAI (GPT-4, GPT-4o, o1)

```yaml
config:
  provider: openai
  model: gpt-4o

  # Function calling
  functions:
    - name: get_weather
      description: Get weather for a location
      parameters:
        type: object
        properties:
          location:
            type: string

  # Structured output
  responseFormat:
    type: json_schema

  # Reasoning models (o1)
  model: o1-preview
  temperature: 1.0  # Fixed for o1
```

### Anthropic (Claude 3.5)

```yaml
config:
  provider: anthropic
  model: claude-3-5-sonnet-20241022

  # Extended thinking (new feature)
  thinkingBudget: 5000  # tokens for internal reasoning

  # System prompt (required for Claude)
  systemPrompt: "You are an expert analyst."
```

### Workers AI (Edge Models)

```yaml
config:
  provider: workers-ai
  model: "@cf/meta/llama-3.1-8b-instruct"
  routing: cloudflare  # Platform-native

  # No API key needed!
  # Uses Workers AI binding
```

### Groq (Ultra-Fast)

```yaml
config:
  provider: groq
  model: llama-3.1-70b-versatile
  routing: cloudflare-gateway

  # Extremely fast inference (< 500ms)
  # Good for real-time applications
```

## Common Patterns

### Classification

```yaml
name: classify-intent
type: Think

config:
  provider: workers-ai
  model: "@cf/meta/llama-3.1-8b-instruct"
  routing: cloudflare
  temperature: 0.2
  maxTokens: 50
  systemPrompt: |
    Classify user intent into: question, request, complaint, or praise.
    Respond with only one word.
```

### Summarization

```yaml
name: summarize-article
type: Think

config:
  provider: openai
  model: gpt-4o-mini
  routing: cloudflare-gateway
  temperature: 0.5
  maxTokens: 200
  systemPrompt: |
    Summarize the article in 2-3 sentences.
    Focus on key points and main ideas.
```

### Extraction

```yaml
name: extract-company-info
type: Think

config:
  provider: anthropic
  model: claude-3-5-sonnet-20241022
  routing: cloudflare-gateway
  temperature: 0.1
  maxTokens: 500
  responseFormat:
    type: json_object
  systemPrompt: |
    Extract company information from the text.
    Return JSON with: name, industry, location, employees, founded.
```

### Generation

```yaml
name: generate-blog-post
type: Think

config:
  provider: openai
  model: gpt-4o
  routing: cloudflare-gateway
  temperature: 0.8
  maxTokens: 2000
  systemPrompt: |
    Write an engaging blog post on the given topic.
    Include:
    - Attention-grabbing headline
    - Introduction with hook
    - 3-5 main points with examples
    - Conclusion with call-to-action
```

### Question Answering (RAG)

```yaml
name: answer-question
type: Think

config:
  provider: anthropic
  model: claude-3-5-sonnet-20241022
  routing: cloudflare-gateway
  temperature: 0.3
  maxTokens: 500
  systemPrompt: |
    Answer the question based only on the provided context.
    If the answer isn't in the context, say "I don't know."

    Context: ${input.context}
```

## Cost Optimization

### Use Cheaper Models

```yaml
# ✅ Good - use mini for simple tasks
- member: classify-email
  config:
    model: gpt-4o-mini  # $0.15/1M tokens

# ❌ Wasteful - using flagship for simple task
- member: classify-email
  config:
    model: gpt-4o  # $5/1M tokens
```

### Aggressive Caching

```yaml
- member: analyze-sentiment
  cache:
    ttl: 86400  # 24 hours
  config:
    routing: cloudflare-gateway  # Persistent cache
```

### Lower Temperature

```yaml
config:
  temperature: 0.1  # More deterministic = better cache hit rate
```

### Limit Token Usage

```yaml
config:
  maxTokens: 100  # Only what you need
  systemPrompt: "Be concise. Maximum 50 words."
```

## Performance Tips

### Use Workers AI for Speed

```yaml
# Sub-50ms cold start, sub-10ms warm
config:
  provider: workers-ai
  model: "@cf/meta/llama-3.1-8b-instruct"
  routing: cloudflare
```

### Use Groq for Fast Inference

```yaml
# ~200ms response time
config:
  provider: groq
  model: llama-3.1-70b-versatile
```

### Use AI Gateway for Caching

```yaml
# Cached responses in < 5ms
config:
  routing: cloudflare-gateway
cache:
  ttl: 3600
```

## Error Handling

### Retry on Failure

```yaml
- member: generate-content
  retry:
    maxAttempts: 3
    backoff: exponential
```

### Scoring with Retry

```yaml
- member: generate-content
  scoring:
    thresholds:
      minimum: 0.7
    onFailure: retry
    retryLimit: 5
```

### Fallback Member

```yaml
flow:
  - member: primary-ai
    continue_on_error: true

  - member: fallback-ai
    condition: ${!primary-ai.success}
```

## Testing Think Members

```typescript
import { describe, it, expect } from 'vitest';
import { TestConductor } from '@ensemble-edge/conductor/testing';

describe('analyze-sentiment', () => {
  it('should analyze positive sentiment', async () => {
    const conductor = await TestConductor.create({
      mocks: {
        ai: {
          responses: {
            'analyze-sentiment': {
              sentiment: 'positive',
              confidence: 0.95
            }
          }
        }
      }
    });

    const result = await conductor.executeMember('analyze-sentiment', {
      text: 'I love this product!'
    });

    expect(result).toBeSuccessful();
    expect(result.output.sentiment).toBe('positive');
  });
});
```

## Best Practices

1. **Use AI Gateway** - Always route through gateway for caching and analytics
2. **Lower temperature for consistency** - Use 0.1-0.3 for deterministic tasks
3. **Limit tokens** - Only request what you need
4. **Cache aggressively** - Long TTLs for stable queries
5. **Use structured output** - JSON schemas for type safety
6. **Test with mocks** - Fast, reliable tests
7. **Monitor costs** - Check AI Gateway dashboard regularly
8. **Choose right model** - Mini for simple, Flagship for complex

## Related Documentation

<CardGroup cols={2}>
  <Card
    title="Routing Modes"
    icon="route"
    href="/conductor/concepts/routing"
  >
    Learn about AI Gateway and routing strategies
  </Card>

  <Card
    title="Caching"
    icon="database"
    href="/conductor/concepts/caching"
  >
    Optimize costs with caching
  </Card>

  <Card
    title="AI Integration Guide"
    icon="brain"
    href="/conductor/guides/ai-integration"
  >
    Setup AI providers
  </Card>

  <Card
    title="Scoring"
    icon="check"
    href="/conductor/concepts/scoring"
  >
    Quality validation with automatic retry
  </Card>
</CardGroup>
